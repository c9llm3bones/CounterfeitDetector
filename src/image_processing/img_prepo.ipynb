{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a75c9c1",
   "metadata": {},
   "source": [
    "## IMAGE PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f105f7c",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d9429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры (подкорректируй)\n",
    "IMAGE_DIR = \"/path/to/images\"      # папка с {ItemID}.png\n",
    "OUT_H5 = \"image_features.h5\"       # куда сохранять фичи\n",
    "BATCH_SIZE = 32                    # можно увеличить/уменьшить\n",
    "NUM_WORKERS = 6\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMG_SHORTER_SIDE = 256             # для Resize(256) -> CenterCrop(224)\n",
    "CROP_SIZE = 224\n",
    "BACKBONE = \"resnet50\"              # можно поменять на efficientnet_b3/clip позже\n",
    "DTYPE = np.float32                 # dtype для features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb5abd9",
   "metadata": {},
   "source": [
    "Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d2df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "from torch.cuda.amp import autocast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d2c03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemImageDataset(Dataset):\n",
    "    def __init__(self, items: list, image_dir: str, transform=None):\n",
    "        \"\"\"\n",
    "        items: list of tuples (item_id, filename) OR list of filenames where basename is ItemID\n",
    "        image_dir: root dir\n",
    "        transform: torchvision transforms\n",
    "        \"\"\"\n",
    "        self.image_dir = Path(image_dir)\n",
    "        # normalize items into list of (id, path)\n",
    "        normalized = []\n",
    "        for it in items:\n",
    "            if isinstance(it, tuple) and len(it) >= 2:\n",
    "                item_id, fname = it[0], it[1]\n",
    "            else:\n",
    "                p = Path(it)\n",
    "                fname = p.name\n",
    "                item_id = p.stem\n",
    "            normalized.append((item_id, str(self.image_dir / fname)))\n",
    "        self.items = normalized\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item_id, fp = self.items[idx]\n",
    "        img = Image.open(fp).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return item_id, img\n",
    "\n",
    "# Получаем список файлов: если у тебя csv с id -> filename, загрузить и собрать список.\n",
    "# Простой fallback: взять все файлы в папке\n",
    "image_dir = Path(IMAGE_DIR)\n",
    "files = [p.name for p in sorted(image_dir.iterdir()) if p.is_file() and p.suffix.lower() in {\".png\",\".jpg\",\".jpeg\",\".webp\"}]\n",
    "items = [(Path(fn).stem, fn) for fn in files]   # (ItemID, filename)\n",
    "print(\"Total images:\", len(items))\n",
    "\n",
    "# Трансформы: Resize(256) + CenterCrop(224) — не искажает пропорции\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SHORTER_SIDE),\n",
    "    transforms.CenterCrop(CROP_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])\n",
    "dataset = ItemImageDataset(items, IMAGE_DIR, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9192b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем resnet50 и отрезаем классификатор\n",
    "model_full = torchvision.models.resnet50(pretrained=True)\n",
    "# убрать последний fc, оставить avgpool -> [B,2048,1,1]\n",
    "backbone = torch.nn.Sequential(*list(model_full.children())[:-1])\n",
    "backbone.to(DEVICE)\n",
    "backbone.eval()\n",
    "\n",
    "# сколько элементов и размер фичи\n",
    "N = len(dataset)\n",
    "with torch.no_grad():\n",
    "    # небольшой прогон одного батча, чтобы убедиться в размерности (без grad)\n",
    "    item0, img0 = dataset[0]\n",
    "    tmp = img0.unsqueeze(0).to(DEVICE)\n",
    "    with autocast():\n",
    "        feat0 = backbone(tmp)\n",
    "    feat_shape = feat0.shape  # (1, 2048, 1, 1) обычно\n",
    "    feat_dim = int(np.prod(feat_shape[1:]))  # 2048\n",
    "print(\"features dim:\", feat_dim)\n",
    "\n",
    "# подготовка HDF5 (создаем datasets)\n",
    "h5f = h5py.File(OUT_H5, \"w\")\n",
    "h5_feats = h5f.create_dataset(\"features\", shape=(N, feat_dim), dtype=DTYPE)\n",
    "# Сохраним ids — попытка привести к int, иначе сохраняем как fixed-length ascii\n",
    "try:\n",
    "    ids_np = np.array([int(t[0]) for t in items], dtype=np.int64)\n",
    "    h5_ids = h5f.create_dataset(\"ids\", data=ids_np, dtype=np.int64)\n",
    "    ids_stored_as_int = True\n",
    "except Exception:\n",
    "    # fallback to bytes\n",
    "    ids_bytes = np.array([str(t[0]).encode(\"utf-8\") for t in items], dtype='S')\n",
    "    h5_ids = h5f.create_dataset(\"ids\", data=ids_bytes, dtype='S')\n",
    "    ids_stored_as_int = False\n",
    "\n",
    "print(\"HDF5 created:\", OUT_H5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd457f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ячейка D: извлечение фичей и запись в HDF5\n",
    "start_idx = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader, desc=\"Feature extraction\"):\n",
    "        item_ids, imgs = batch  # item_ids: list of ids as strings, imgs: tensor [B,C,H,W]\n",
    "        imgs = imgs.to(DEVICE, non_blocking=True)\n",
    "        with autocast():  # ускорение и уменьшение consumption VRAM\n",
    "            feats = backbone(imgs)  # [B, 2048, 1, 1]\n",
    "            feats = feats.view(feats.size(0), -1)  # [B, feat_dim]\n",
    "        feats_cpu = feats.cpu().numpy().astype(DTYPE)\n",
    "        b = feats_cpu.shape[0]\n",
    "        h5_feats[start_idx:start_idx+b] = feats_cpu\n",
    "        start_idx += b\n",
    "\n",
    "# ensure flush & close\n",
    "h5f.flush()\n",
    "h5f.close()\n",
    "print(\"Done. Saved features for\", start_idx, \"images.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
