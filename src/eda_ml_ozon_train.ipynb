{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fbfa90e",
   "metadata": {},
   "source": [
    "\n",
    "# üìò EDA ‚Äî `ml_ozon_counterfeit_train.csv`\n",
    "\n",
    "–≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫ ‚Äî —à–∞–±–ª–æ–Ω –¥–ª—è –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —Ö–∞–∫–∞—Ç–æ–Ω–∞.\n",
    "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ CSV –∏ (–ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏) –∫ –∫–∞—Ç–∞–ª–æ–≥—É —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –≤ —Å–µ–∫—Ü–∏–∏ **Config** –Ω–∏–∂–µ.\n",
    "\n",
    "**–ß—Ç–æ –¥–µ–ª–∞–µ—Ç –Ω–æ—É—Ç–±—É–∫:**\n",
    "- –ó–∞–≥—Ä—É–∂–∞–µ—Ç CSV –Ω–∞–¥—ë–∂–Ω–æ (–ø–æ–∏—Å–∫ —Ñ–∞–π–ª–∞ –ø–æ –º–∞—Å–∫–µ, –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä—É—Å—Å–∫–æ–π/–ª–∞—Ç–∏–Ω—Å–∫–æ–π `c/—Å` –≤ –∏–º–µ–Ω–∏).\n",
    "- –ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Ä–∞–∑–º–µ—Ä—ã, —Ç–∏–ø—ã –∫–æ–ª–æ–Ω–æ–∫, –ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏.\n",
    "- –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∏ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –º–µ—Ç–∫–∏ `resolution`.\n",
    "- –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å `id`.\n",
    "- –û—Ç—á—ë—Ç –ø–æ –ø—Ä–æ–ø—É—Å–∫–∞–º.\n",
    "- –í—ã–¥–µ–ª—è–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ, –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∏ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.\n",
    "- –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –ø–æ–ª–µ–π (–¥–ª–∏–Ω—ã —Å—Ç—Ä–æ–∫, –ø—Ä–∏–º–µ—Ä –∑–Ω–∞—á–µ–Ω–∏–π).\n",
    "- –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –∏—Ö —Å–≤—è–∑—å —Å —Ç–∞—Ä–≥–µ—Ç–æ–º.\n",
    "- –ü–æ–∏—Å–∫ –∫–æ–ª–æ–Ω–æ–∫, —É–∫–∞–∑—ã–≤–∞—é—â–∏—Ö –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, `image`, `img_path`, `picture`).\n",
    "\n",
    "> –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –≥—Ä–∞—Ñ–∏–∫–∏ —Å—Ç—Ä–æ—è—Ç—Å—è —á–µ—Ä–µ–∑ Matplotlib (–±–µ–∑ seaborn).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a37d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Config ===\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "# –£–∫–∞–∂–∏ –ø—É—Ç—å –∫ –∫–∞—Ç–∞–ª–æ–≥—É —Å –¥–∞–Ω–Ω—ã–º–∏ (–≥–¥–µ –ª–µ–∂–∞—Ç CSV)\n",
    "DATA_DIR = Path('.')  # –Ω–∞–ø—Ä–∏–º–µ—Ä, Path('/kaggle/input/dataset') –∏–ª–∏ Path('data/raw')\n",
    "\n",
    "# –Ø–≤–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞ (–µ—Å–ª–∏ –∑–Ω–∞–µ—à—å —Ç–æ—á–Ω–æ–µ). –ò–Ω–∞—á–µ –æ—Å—Ç–∞–≤—å None, –∏ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ–∏—Å–∫ –ø–æ –º–∞—Å–∫–µ.\n",
    "TRAIN_CSV_NAME = None  # –Ω–∞–ø—Ä–∏–º–µ—Ä, 'ml_ozon_counterfeit_train.csv'\n",
    "\n",
    "# –ü—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –∫–∞—Ä—Ç–∏–Ω–æ–∫ –º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –ø–∞–ø–∫–∏ (–ø–æ –∂–µ–ª–∞–Ω–∏—é, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø—É—Ç–µ–π)\n",
    "IMAGES_TRAIN_DIR = None  # –Ω–∞–ø—Ä–∏–º–µ—Ä, Path('data/images_train')\n",
    "\n",
    "# –ú–∞—Å–∫–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ train-—Ñ–∞–π–ª–∞, —É—á–∏—Ç—ã–≤–∞—è –≤–æ–∑–º–æ–∂–Ω—É—é –∫–∏—Ä–∏–ª–ª–∏—Ü—É/–ª–∞—Ç–∏–Ω–∏—Ü—É –≤ —Å–ª–æ–≤–µ \"counterfeit\"\n",
    "TRAIN_GLOBS = [\n",
    "    'ml_ozon_*train*.csv',\n",
    "    'ml_ozon*train*.csv',\n",
    "]\n",
    "\n",
    "def resolve_train_csv():\n",
    "    if TRAIN_CSV_NAME is not None:\n",
    "        p = (DATA_DIR / TRAIN_CSV_NAME)\n",
    "        assert p.exists(), f'–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª {p}'\n",
    "        return p\n",
    "    # –ü–æ–∏—Å–∫ –ø–æ –º–∞—Å–∫–∞–º\n",
    "    candidates = []\n",
    "    for pat in TRAIN_GLOBS:\n",
    "        candidates.extend(glob(str(DATA_DIR / pat)))\n",
    "    # –£–¥–∞–ª–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ –æ—Ç—Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –¥–ª–∏–Ω–µ (–∫–æ—Ä–æ—á–µ ‚Äî –≤–µ—Ä–æ—è—Ç–Ω–µ–µ –Ω—É–∂–Ω—ã–π)\n",
    "    candidates = sorted(set(candidates), key=len)\n",
    "    assert candidates, f'–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ train CSV –ø–æ –º–∞—Å–∫–∞–º {TRAIN_GLOBS} –≤ {DATA_DIR.resolve()}'\n",
    "    print('–ö–∞–Ω–¥–∏–¥–∞—Ç—ã train CSV:', *candidates, sep='\\n- ')\n",
    "    return Path(candidates[0])\n",
    "\n",
    "TRAIN_CSV_PATH = resolve_train_csv()\n",
    "TRAIN_CSV_PATH\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f98fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Imports ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e664b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Load CSV ===\n",
    "# –ü—ã—Ç–∞–µ–º—Å—è —á–∏—Ç–∞—Ç—å –≤ UTF-8; –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏—Ç—Å—è ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º cp1251.\n",
    "read_kwargs = dict(low_memory=False)\n",
    "try:\n",
    "    train = pd.read_csv(TRAIN_CSV_PATH, **read_kwargs)\n",
    "except UnicodeDecodeError:\n",
    "    train = pd.read_csv(TRAIN_CSV_PATH, encoding='cp1251', **read_kwargs)\n",
    "\n",
    "print('Shape:', train.shape)\n",
    "train.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b48c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Info & dtypes ===\n",
    "print(train.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13894a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Target check: 'resolution' ===\n",
    "if 'resolution' in train.columns:\n",
    "    y = train['resolution'].astype(int)\n",
    "    print('Target value counts:\\n', y.value_counts(dropna=False))\n",
    "    print('\\nClass ratio (positive rate):', y.mean().round(4))\n",
    "    # Bar chart\n",
    "    vc = y.value_counts().sort_index()\n",
    "    plt.figure()\n",
    "    vc.plot(kind='bar', title='Target distribution (resolution)')\n",
    "    plt.xlabel('class')\n",
    "    plt.ylabel('count')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –∫–æ–ª–æ–Ω–∫–∞ 'resolution' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ train. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Ü–µ–ª–µ–≤–æ–π –º–µ—Ç–∫–∏.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5943656b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === ID uniqueness ===\n",
    "id_col = None\n",
    "for cand in ['id', 'item_id', 'product_id']:\n",
    "    if cand in train.columns:\n",
    "        id_col = cand\n",
    "        break\n",
    "\n",
    "if id_col:\n",
    "    uniq = train[id_col].nunique()\n",
    "    total = len(train)\n",
    "    print(f\"ID column: {id_col}. Unique: {uniq} / {total}. Duplicates: {total - uniq}\")\n",
    "    if total - uniq > 0:\n",
    "        dup = train[train.duplicated(id_col, keep=False)].sort_values(id_col)\n",
    "        print('–ü—Ä–∏–º–µ—Ä—ã –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –ø–æ ID:')\n",
    "        display(dup.head(10))\n",
    "else:\n",
    "    print('‚ö†Ô∏è –ö–æ–ª–æ–Ω–∫–∞ ID –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ —Å—Ä–µ–¥–∏ —Ç–∏–ø–∏—á–Ω—ã—Ö –∏–º—ë–Ω (id/item_id/product_id). –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Ä—É—á–Ω—É—é.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874a2c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Missing values ===\n",
    "mv = train.isna().sum().sort_values(ascending=False)\n",
    "mv = mv[mv > 0]\n",
    "if len(mv) == 0:\n",
    "    print('–ü—Ä–æ–ø—É—Å–∫–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.')\n",
    "else:\n",
    "    miss = pd.DataFrame({'missing': mv, 'missing_%': (mv / len(train) * 100).round(2)})\n",
    "    display(miss.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e9fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Detect likely text columns ===\n",
    "obj_cols = [c for c in train.columns if train[c].dtype == 'object']\n",
    "text_candidates = []\n",
    "for c in obj_cols:\n",
    "    # –û—Ü–µ–Ω–∏–º —Å—Ä–µ–¥–Ω—é—é –¥–ª–∏–Ω—É —Å—Ç—Ä–æ–∫, –¥–æ–ª—é —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏ –∫–æ–ª-–≤–æ NaN\n",
    "    s = train[c].astype(str)\n",
    "    lens = s.str.len()\n",
    "    avg_len = lens.mean()\n",
    "    uniq_frac = s.nunique(dropna=True) / max(len(s), 1)\n",
    "    nan_rate = train[c].isna().mean()\n",
    "    text_candidates.append((c, avg_len, uniq_frac, nan_rate))\n",
    "\n",
    "text_df = pd.DataFrame(text_candidates, columns=['column', 'avg_len', 'uniq_frac', 'nan_rate']).sort_values('avg_len', ascending=False)\n",
    "display(text_df.head(20))\n",
    "\n",
    "# –í—ã–±–µ—Ä–µ–º –≤–µ—Ä–æ—è—Ç–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è (—Å—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ >= 20 —Å–∏–º–≤–æ–ª–æ–≤)\n",
    "likely_text_cols = text_df[text_df['avg_len'] >= 20]['column'].tolist()\n",
    "print('–í–µ—Ä–æ—è—Ç–Ω—ã–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏:', likely_text_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614ad1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Text length histograms (–ø–µ—Ä–≤—ã–µ 2) ===\n",
    "for c in likely_text_cols[:2]:\n",
    "    lens = train[c].astype(str).str.len()\n",
    "    plt.figure()\n",
    "    lens.hist(bins=50)\n",
    "    plt.title(f'Text length distribution ‚Äî {c}')\n",
    "    plt.xlabel('length (chars)')\n",
    "    plt.ylabel('count')\n",
    "    plt.show()\n",
    "    print(train[c].dropna().head(3).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fcb9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Categorical candidates ===\n",
    "cat_candidates = []\n",
    "for c in obj_cols:\n",
    "    nunq = train[c].nunique(dropna=True)\n",
    "    frac = nunq / max(len(train), 1)\n",
    "    cat_candidates.append((c, nunq, frac))\n",
    "cat_df = pd.DataFrame(cat_candidates, columns=['column', 'nunique', 'unique_frac']).sort_values(['nunique','column'])\n",
    "# –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –Ω–∏–∑–∫–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å (<500 –∏ <50% —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏)\n",
    "likely_cat = cat_df[(cat_df['nunique'] <= 500) & (cat_df['unique_frac'] <= 0.5)]['column'].tolist()\n",
    "display(cat_df.head(30))\n",
    "print('–í–µ—Ä–æ—è—Ç–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ:', likely_cat[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48481ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Numeric columns analysis ===\n",
    "num_cols = [c for c in train.columns if pd.api.types.is_numeric_dtype(train[c]) and c != 'resolution']\n",
    "if num_cols:\n",
    "    display(train[num_cols].describe().T)\n",
    "else:\n",
    "    print('–ß–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ (–∫—Ä–æ–º–µ —Ç–∞—Ä–≥–µ—Ç–∞, –µ—Å–ª–∏ –æ–Ω —á–∏—Å–ª–æ–≤–æ–π).')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ae00dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Correlation with target (numeric vs binary target) ===\n",
    "if 'resolution' in train.columns and len(num_cols) > 0:\n",
    "    corr_rows = []\n",
    "    y = train['resolution'].astype(float)\n",
    "    for c in num_cols:\n",
    "        s = pd.to_numeric(train[c], errors='coerce')\n",
    "        mask = ~s.isna() & ~y.isna()\n",
    "        if mask.sum() >= 50:  # –º–∏–Ω–∏–º—É–º –Ω–∞–±–ª—é–¥–µ–Ω–∏–π\n",
    "            corr = np.corrcoef(s[mask], y[mask])[0,1]\n",
    "            corr_rows.append((c, corr))\n",
    "    corr_df = pd.DataFrame(corr_rows, columns=['column', 'pearson_corr_with_target']).sort_values('pearson_corr_with_target', ascending=False)\n",
    "    display(corr_df.head(20))\n",
    "else:\n",
    "    print('–ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—é: –Ω–µ—Ç —Ç–∞—Ä–≥–µ—Ç–∞ –∏–ª–∏ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd840a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Image path columns detection ===\n",
    "image_like_cols = [c for c in train.columns if any(k in c.lower() for k in ['image', 'img', 'picture', 'photo', 'pic', 'jpg', 'png', 'path'])]\n",
    "print('–ü–æ—Ö–æ–∂–∏–µ –Ω–∞ –ø—É—Ç–∏ –∫ –∫–∞—Ä—Ç–∏–Ω–∫–∞–º –∫–æ–ª–æ–Ω–∫–∏:', image_like_cols)\n",
    "if image_like_cols:\n",
    "    for c in image_like_cols[:3]:\n",
    "        print(f'–ö–æ–ª–æ–Ω–∫–∞ {c}: –ø—Ä–∏–º–µ—Ä –∑–Ω–∞—á–µ–Ω–∏–π ->')\n",
    "        print(train[c].dropna().astype(str).head(5).tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9206d84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === High-cardinality id-like columns ===\n",
    "id_like = []\n",
    "for c in train.columns:\n",
    "    nunq = train[c].nunique(dropna=True)\n",
    "    if nunq > 0.9 * len(train):\n",
    "        id_like.append(c)\n",
    "print('ID-like/high-cardinality columns:', id_like)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4625af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Save quick summary JSON ===\n",
    "summary = {\n",
    "    'shape': tuple(train.shape),\n",
    "    'columns': train.columns.tolist(),\n",
    "    'dtypes': {c: str(train[c].dtype) for c in train.columns},\n",
    "    'has_resolution': 'resolution' in train.columns,\n",
    "}\n",
    "summary_path = Path('dataset_summary.json')\n",
    "with open(summary_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "print('Saved summary to', summary_path.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cef12a2",
   "metadata": {},
   "source": [
    "\n",
    "## ‚úÖ Next steps\n",
    "1. –£—Ç–æ—á–Ω–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã: `id`, –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç (`description`/`name`/`title`), –ø—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º, –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ/—á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏.\n",
    "\n",
    "2. –ó–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞—Ç—å —Å–ø–∏—Å–∫–∏ –∫–æ–ª–æ–Ω–æ–∫ –ø–æ —Ç–∏–ø–∞–º (text/cat/num) –¥–ª—è –ø–æ—Å–ª–µ–¥—É—é—â–µ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞.\n",
    "\n",
    "3. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∏—Å–±–∞–ª–∞–Ω—Å –ø–æ —Ü–µ–ª–µ–≤–æ–π –º–µ—Ç–∫–µ –∏ —Ä–µ—à–∏—Ç—å, –Ω—É–∂–µ–Ω –ª–∏ —Ç—é–Ω–∏–Ω–≥ –ø–æ—Ä–æ–≥–∞.\n",
    "\n",
    "4. –ü–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å stratifed —Å–ø–ª–∏—Ç—ã –∏ OOF-–≤–∞–ª–∏–¥–∞—Ü–∏—é –ø–æ F1.\n",
    "\n",
    "5. –ü–µ—Ä–µ–π—Ç–∏ –∫ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—é –±–∞–∑–æ–≤–æ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ (TF-IDF + –ª–æ–≥—Ä–µ–≥/LinearSVC) –∫–∞–∫ —Å—Ç–∞—Ä—Ç–æ–≤–æ–π —Ç–æ—á–∫–∏.\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
